[{"body": ""}
, {"url": "https://blog.inductorsoftware.com/Perplexity/home/devhowto/devhowtoOverview", "excerpt":"Overview\nThe DELPH-IN Consortium has developed a large and rich set of technologies for manipulating natural language. The list of DELPH-IN applications is a great place to start for a survey of different approaches to using them.  There you will find pointers to uses such as identification of speculative or negated event mentions in biomedical text (MacKinlay et al 2011), question generation (Yao et al 2012), detecting the scope of negation (Packard et al 2014), relating natural language to robot control language (Packard 2014), and recognizing textual entailment (PETE task; Lien & Kouylekov 2015). ERS representations have also been beneficial in semantic transfer-based MT (Oepen et al 2007, Bond et al 2011), ontology acquisition (Herbelot & Copestake 2006), extraction of glossary sentences (Reiplinger et al 2012), sentiment analysis (Kramer & Gordon 2014), and the ACL Anthology Searchbench (Sch\u00e4fer et al 2011), and many more.\nThis tutorial is designed to show developers how to consume a narrow set of DELPH-IN technologies (especially MRS and ACE) to build an application. It focuses on one particular application (a natural language interface to a computer's file system), but the concepts should apply to any type of constrained system ('constrained' in the sense of the size of the world under discussion). It also takes one particular approach to building the system by logically evaluating the output of the DELPH-IN parsers against a world definition. While this approach may not be the right one for every application, the concepts illustrated and the tools used along the way should be more broadly applicable.\nThe tutorial will use the DELPH-IN English Resource Grammar (ERG) to parse English, but the concepts are the same across the DELPH-IN grammars.  In fact, the library functions we build have no dependency on the grammar at all. They can be used for any of the DELPH-IN grammars.\nPython was chosen as a simple, popular, open-source language available on many platforms. However, the examples and approach shown here could be implemented in any language. There is not much code in the core solver and associated helper functions that would need to be translated. The overwhelming majority of code will be in the implementation of the terms you implement for your own domain.\nIt is designed to be read in order, but the most important background is in the first two sections, The Minimal Recursion Semantics (MRS) Format and Building Well-Formed MRS Trees. These should definitely be read before moving on to the rest of the topics. \nComprehensive source for the completed tutorial is available here.\n\n", "title":"Overview", "teaser":"Overview\nThe DELPH-IN Consortium has developed a large and rich set of technologies for manipulating natural language. The list of DELPH-IN applicatio ...", "site":"Perplexity", "section":"How-To", "categories":"", "tags":""}
, {"url": "https://blog.inductorsoftware.com/Perplexity/home/devhowto/devhowtoMRS", "excerpt":"The Minimal Recursion Semantics (MRS) Format\nThis section provides an overview of the Minimal Recursion Semantics format which is the primary artifact used by DELPH-IN to represent the meaning of a phrase. It should be sufficient for understanding all of the rest of the material in the tutorial.  For a deeper dive into MRS, explore Minimal Recursion Semantics: An Introduction.\n\nThe DELPH-IN English Resource Grammar (ERG), via the ACE parser, converts an English phrase into a text format called the \"Minimal Recursion Semantics\" (MRS) format which is designed to allow software to process human language. ACE can also be used with any of the other DELPH-IN grammars to convert other natural languages into the MRS format. While the examples below use English, the concepts apply across the DELPH-IN grammars.\nBecause language is ambiguous, most phrases parse into more than one MRS document, each representing a different interpretation of the phrase. Each MRS document encodes one high-level meaning of the phrase into a list of predicate-logic-like predicates (called predications).\nEach MRS document also has multiple interpretations. Using constraints that are included as part of the MRS, a set of trees (called well-formed trees) can be built from the flat list of predications in a given MRS.  These well-formed trees define all the alternative meanings of that particular MRS.\nSo, a phrase generates n MRS documents, each of those generates m well-formed trees, which results in n x m possible interpretations of a single phrase. One of the challenges of building a system that uses natural language is to determine which of the many possible meanings was intended by the user (one approach to doing this will be discussed in a future section of the tutorial).\nFor example, the phrase: \"Look under the table.\" produces 12 different MRS documents (also called \"parses\" or \"interpretations\"). These include interpretations that mean: \n\"Look (at whatever is) under the table\" \n\n\"Look (around while you are) under the table\" \n\n... among 10 others. \nThe MRS document for the first interpretation is:\n[ TOP: h0\nINDEX: e2\nRELS: &lt; \n[ _the_q LBL: h10 ARG0: x9 [ x PERS: 3 NUM: sg IND: + ] RSTR: h11 BODY: h12 ]\n[ _table_n_1 LBL: h13 ARG0: x9 [ x PERS: 3 NUM: sg IND: + ] ]\n[ pronoun_q LBL: h4 ARG0: x3 [ x PERS: 2 PT: zero ] RSTR: h5 BODY: h6 ]\n[ pron LBL: h7 ARG0: x3 [ x PERS: 2 PT: zero ] ]\n[ _under_p_dir LBL: h1 ARG0: e8 [ e SF: prop TENSE: untensed MOOD: indicative PROG: - PERF: - ] ARG1: e2 ARG2: x9 ]\n[ _look_v_1 LBL: h1 ARG0: e2 [ e SF: comm TENSE: pres MOOD: indicative PROG: - PERF: - ] ARG1: x3 ]\n&gt;\nHCONS: &lt; h0 qeq h1 h5 qeq h7 h11 qeq h13 &gt; ]\n\nUsing the constraints described in the HCONS section (which we will describe later), there are two well-formed trees that can be built from that MRS, which describe the two alternatives that it could mean:\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500 _table_n_1(x9)\n_the_q(x9,RSTR,BODY)               \u250c\u2500\u2500\u2500\u2500\u2500\u2500 pron(x3)\n                 \u2514\u2500 pronoun_q(x3,RSTR,BODY)    \u250c\u2500\u2500 _under_p_dir(e8,e2,x9)\n                                        \u2514\u2500 and(0,1)\n                                                 \u2514 _look_v_1(e2,x3)\n\n               \u250c\u2500\u2500\u2500\u2500\u2500\u2500 pron(x3)\npronoun_q(x3,RSTR,BODY)            \u250c\u2500\u2500\u2500\u2500\u2500\u2500 _table_n_1(x9)\n                    \u2514\u2500 _the_q(x9,RSTR,BODY)    \u250c\u2500\u2500 _under_p_dir(e8,e2,x9)\n                                        \u2514\u2500 and(0,1)\n                                                 \u2514 _look_v_1(e2,x3)\n\nThe rest of this section will give you a base understanding of the MRS format so that we can explore how to build these well-formed trees in a later section and ultimately write software that derives the speaker's intended meaning from them.  Deriving their intended meaning is the topic of this entire tutorial.\nUnderspecification\nA DELPH-IN parser like ACE will usually generate more than one MRS document representing the various high-level interpretations of a phrase. Each one contains a list of predicate-logic-like predications and not a tree like you'll see in many natural language systems.  That's because it is underspecified.  Even though the parser has already done one level of interpretation on the phrase, there are still (usually) multiple ways to interpret that.  \nThe final interpretations of a phrase are called \"well-formed MRS trees\". The MRS document doesn't pick a primary interpretation by choosing a specific tree, it provides the rules for building all of them. That's what \"underspecified\" means. Every book is in a cave could mean \"all books are in the same cave\" or \"every book is in a (possibly different) cave\". Given just the phrase, it isn't clear which the use intended, so the MRS provides all the alternatives. Context (which the MRS doesn't have) usually helps to decide which is meant.\nThis section will go through the entire MRS document in detail, but as an overview: The list of predicate-logic-like predications in provided in the RELS section of the MRS document:\n...\n\nRELS: &lt; \n[ _the_q LBL: h10 ARG0: x9 [ x PERS: 3 NUM: sg IND: + ] RSTR: h11 BODY: h12 ]\n[ _table_n_1 LBL: h13 ARG0: x9 [ x PERS: 3 NUM: sg IND: + ] ]\n[ pronoun_q LBL: h4 ARG0: x3 [ x PERS: 2 PT: zero ] RSTR: h5 BODY: h6 ]\n[ pron LBL: h7 ARG0: x3 [ x PERS: 2 PT: zero ] ]\n[ _under_p_dir LBL: h1 ARG0: e8 [ e SF: prop TENSE: untensed MOOD: indicative PROG: - PERF: - ] ARG1: e2 ARG2: x9 ]\n[ _look_v_1 LBL: h1 ARG0: e2 [ e SF: comm TENSE: pres MOOD: indicative PROG: - PERF: - ] ARG1: x3 ]\n&gt;\n\n...\n\n... and the HCONS section lists the constraints on putting the predications together to create a well-formed tree which represents a single meaning:\n... \n\nHCONS: &lt; h0 qeq h1 h5 qeq h7 h11 qeq h13 &gt; ]\n\nThe MRS is underspecified, and the RELS together with the HCONS provide the information to make it specific and recover the various possible meanings.\nPredications\nA phrase is converted into a list of predicate-logic like predications in the MRS which you can see in the RELS section of the MRS for \"Look under the table\":\n...\n\nRELS: &lt; \n[ _the_q LBL: h10 ARG0: x9 [ x PERS: 3 NUM: sg IND: + ] RSTR: h11 BODY: h12 ]\n[ _table_n_1 LBL: h13 ARG0: x9 [ x PERS: 3 NUM: sg IND: + ] ]\n[ pronoun_q LBL: h4 ARG0: x3 [ x PERS: 2 PT: zero ] RSTR: h5 BODY: h6 ]\n[ pron LBL: h7 ARG0: x3 [ x PERS: 2 PT: zero ] ]\n[ _under_p_dir LBL: h1 ARG0: e8 [ e SF: prop TENSE: untensed MOOD: indicative PROG: - PERF: - ] ARG1: e2 ARG2: x9 ]\n[ _look_v_1 LBL: h1 ARG0: e2 [ e SF: comm TENSE: pres MOOD: indicative PROG: - PERF: - ] ARG1: x3 ]\n&gt;\n\n...\n\nPredications are \"predicate-logic-like\" in that they state a relation or a fact about their arguments that must be true in order for the MRS to be true. The arguments are most often variables and, if you find values for all the variables that make all the predications in the MRS true in a given world, then you have \"solved\" or \"resolved\" the MRS. You have figured out (in a sense) the meaning of the sentence. So, predications do the work in an MRS by providing constraints or restrictions on the variables they are passed. \nFor example: the predication _table_n_1(x9) in the example above is saying \"restrict the set of things in the variable x9 to be only those which are a 'table'\" or, alternatively: \"ensure that x9 contains a 'table'\".  Depending on how you ultimately solve the MRS, you might look at these variables as containing sets or individual items. Our approach will iteratively solve the MRS using individual items, so we'll be describing predications as restricting to individual items for the rest of the tutorial.\nIf we evaluated a different predication such as _large_a_1(x9) immediately afterward, it would mean \"also make sure the thing in x9 is 'large'\".  An MRS that contains both predications like that is saying, \"restrict x9 to be a 'large table' from the world we are talking about\".\nWe'll get into the other examples later after we've covered more basics.\nPredication Labels\nEach predication has a label in the MRS, indicated by LBL:. The label serves as an ID or a pointer to the predication. Note that predications can share the same label. In fact, this is how the MRS indicates they are \"in conjunction\" (i.e. should be interpreted together using a logical \"and\", as in the above example).\nLook at the labels for the different predications in an MRS for \"Look under the large table\" and note that _large_a_1 and _table_n_1 share the same label, indicating they are \"in conjunction\":\n[ TOP: h0\nINDEX: e2\nRELS: &lt; [ _the_q LBL: h10 ARG0: x9 [ x PERS: 3 NUM: sg IND: + ] RSTR: h11 BODY: h12 ]\n[ _large_a_1 LBL: h13 ARG0: e14 [ e SF: prop TENSE: untensed MOOD: indicative PROG: bool PERF: - ] ARG1: x9 ]\n[ _table_n_1 LBL: h13 ARG0: x9 [ x PERS: 3 NUM: sg IND: + ] ]\n[ pronoun_q LBL: h4 ARG0: x3 [ x PERS: 2 PT: zero ] RSTR: h5 BODY: h6 ]\n[ pron LBL: h7 ARG0: x3 [ x PERS: 2 PT: zero ] ]\n[ _under_p_dir LBL: h1 ARG0: e8 [ e SF: prop TENSE: untensed MOOD: indicative PROG: - PERF: - ] ARG1: e2 ARG2: x9 ]\n[ _look_v_1 LBL: h1 ARG0: e2 [ e SF: comm TENSE: pres MOOD: indicative PROG: - PERF: - ] ARG1: x3 ]\n&gt;\nHCONS: &lt; h0 qeq h1 h5 qeq h7 h11 qeq h13 &gt; ]\n\nThese labels are used to turn the flat list of predications into the set of well-formed trees that represent its various meanings. The section below on scopal arguments gives an overview of how this works. The Well-Formed Trees topic describes it in detail.\nPredication Names\nThe name of a predication, for example, _table_n_1, encodes important information about it:\nThe \"lemma\" or root word (this is the first word you see): \"table\"\n\nWhether it was actually seen in the text (starts with _) or added abstractly by the system (no initial _)\n\nIts part of speech. The _n_ in _table_n_1 means \"table\" is a \"noun\". The _q in _the_q means \"the\" is a \"quantifier\" (quantifiers are described below)\n\nIt may have extras at the end like _1 to indicate which \"variant\" or synonym of the word it represents\n\nThere is some documentation for what the predications mean, which can be found by doing a search of the documentation. Otherwise, their meaning can often be determined by looking at the MRS and intuiting what they are trying to do using your knowledge of the language. If all else fails, you can post on the message boards.  \nPredication Arguments and Variables\nPredications have arguments with names like ARG0, ARG1, ARG2, RSTR, BODY, etc.  Think of those exactly like the name of named arguments in some programming languages such as Python.\nThey also have variables assigned to the arguments like x5, h1, e6.  The initial letter in the name indicates the \"type\" of variable it is.  The types create a hierarchy, with the bottommost \"leaves\" being the types that are the most concrete and most common in predications. Each of these types will be discussed below:\n    u\n   / \\\n  i   p\n / \\ / \\\ne   x   h\n\nThe number on a variable just makes it unique. When the same variable name appears in more than one place it is shared, just like if you used a Python variable in more than one place in a function.\nSo, if an MRS has two predications like this:\n[ _large_a_1 LBL: h13 ARG0: e14 [ e SF: prop TENSE: untensed MOOD: indicative PROG: bool PERF: - ] ARG1: x9 ]\n[ _table_n_1 LBL: h13 ARG0: x9 [ x PERS: 3 NUM: sg IND: + ] ]\n\n... you can see that:\n_large_a_1 has two arguments: ARG0 and ARG1, and the variables assigned to them are: e14 and x9. The first is of type event (e) and the second is of type instance (x)\n\n_table_n_1 shares x9 in its ARG0 and so both predications are restricting the same variable. This means that, ultimately, x9 should contain only \"large tables\"\n\nThinking of MRS variables as variables in a math equation can help: The MRS is effectively defining a formula with variables. If you pick variable values such that the MRS is true for a given world, then you have understood the meaning of the MRS in that world.\nOf all the arguments, ARG0 is special.  It holds a variable that \"represents\" the predication, sometimes called the \"characteristic\" or \"distinguished\" variable, but most often the \"instrinsic variable\".  If you read the Minimal Recursion Semantics: An Introduction documentation, you'll see the term \"introduced\" is used to describe the intrinsic variable.  A predicate is described as \"introducing\" its \"intrinsic variable\" (which is always ARG0). Sometimes phrases like \"the variable introduced by predicate X...\" are used.  This will become important later, mostly when we talk about events or about how to convert predications back into a phrase. For now, it is enough to understand that ARG0 represents the predication in some special ways.\nOne final point: Every variable in an MRS is introduced by exactly one predication in the MRS (which is why they can serve as makeshift \"representations\" of the predication). We'll come back to this when we talk about i, p and u variable types.\nH (Handle) Variables, aka \"Scopal Arguments\"\nThe semantic meaning of an MRS is ultimately represented by a tree (described in the next topic) and handle variables passed to predications (aka \"scopal arguments\") provide the mechanism to build a tree from the list of predications.\nHandle variables represent the \"holes\" where branches of the tree can be placed. To do this, handle variables are set to the LBL: of another predication. As described above, the MRS LBL: field serves as a way to \"label\" each predication with a unique identifier. Thus, the LBL: of a predication can be assigned to a handle variable in a different predication to indicate that it should be placed there. By assigning LBL:s to holes like that, an entire tree can be built.\nWhen a tree is built and being resolved, a predication with handle arguments is expected to use those branches to do ... whatever it is supposed to do. For example, the _the_q has two handle arguments, h5 and h6 in the MRS for \"The dog is small\":\n[ TOP: h0\nINDEX: e2\nRELS: &lt; \n[ _the_q LBL: h4 ARG0: x3 [ x PERS: 3 NUM: sg IND: + ] RSTR: h5 BODY: h6 ]\n[ _dog_n_1 LBL: h7 ARG0: x3 [ x PERS: 3 NUM: sg IND: + ] ]\n[ _small_a_1 LBL: h1 ARG0: e2 [ e SF: prop TENSE: pres MOOD: indicative PROG: - PERF: - ] ARG1: x3 ]\n&gt;\nHCONS: &lt; h0 qeq h1 h5 qeq h7 &gt; ]\n\nIn building a tree, we have assigned LBL: h7 to h5 and LBL: h1 to h6:\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500 _dog_n_1(x3)\n_the_q(x3,RSTR,BODY)\n                 \u2514\u2500 _small_a_1(e2,x3)\n\nThink of this process like a lambda function being passed to a function in a programming language like C++ or C#.  The the_q predication itself will be responsible for \"doing something\" with the two branches it is passed.  What, exactly, is specific to the predication. We'll go into this more in a future topic in the tutorial. For now, think about scopal arguments as places to put other predications which are acting like programming language \"lambda functions\".\nBecause the MRS is underspecified, it usually doesn't directly list which predication to put in which scopal argument. You figure that out by the process of creating a well-formed tree.  However, if a predication has a LBL: that is the same handle as a scopal argument, then that part of the tree has been specified and is \"locked in place\" (i.e. there is no hole there for something else to be).\nX (Instance) Variables\nInstance (x) variables are just like normal First Order Logic variables, or like variables in popular programming languages. The types of things they can contain are \"individuals\", which is another name for a \"thing in the world\".  They hold the things the speaker is talking about.\nIn the MRS for \"Look under the table\":\n[ TOP: h0\nINDEX: e2\nRELS: &lt; \n[ _the_q LBL: h10 ARG0: x9 [ x PERS: 3 NUM: sg IND: + ] RSTR: h11 BODY: h12 ]\n[ _large_a_1 LBL: h13 ARG0: e14 [ e SF: prop TENSE: untensed MOOD: indicative PROG: bool PERF: - ] ARG1: x9 ]\n[ _table_n_1 LBL: h13 ARG0: x9 [ x PERS: 3 NUM: sg IND: + ] ]\n[ pronoun_q LBL: h4 ARG0: x3 [ x PERS: 2 PT: zero ] RSTR: h5 BODY: h6 ]\n[ pron LBL: h7 ARG0: x3 [ x PERS: 2 PT: zero ] ]\n[ _under_p_dir LBL: h1 ARG0: e8 [ e SF: prop TENSE: untensed MOOD: indicative PROG: - PERF: - ] ARG1: e2 ARG2: x9 ]\n[ _look_v_1 LBL: h1 ARG0: e2 [ e SF: comm TENSE: pres MOOD: indicative PROG: - PERF: - ] ARG1: x3 ]\n&gt;\nHCONS: &lt; h0 qeq h1 h5 qeq h7 h11 qeq h13 &gt; ]\n\n... there are only two instance variables that represent the \"things in the world being talked about\":\nx9: \"the large table\"\n\nx3: \"you\". This is implied since it is a command. I.e. \"(You) look at the table\". You can tell it wasn't in the original phrase because the predication doesn't start with _.\n\nThe other variables in the MRS are there to help build up the tree (h variables, described previously) or allow predications to refer to each other (e variables, described next).  x variables are the most concrete type of variable that maps most obviously to what is being said in the phrase.\nNote that instance variables are always scoped by a quantifier when a well-formed tree is built. Quantifiers are described later, but for now think of them as a predication named with _q and with the argument structure: (x, h, h). The first argument of the quantifier, x, is the variable being \"scoped\", and the two branches in its scopal arguments are the only branches allowed to use that particular x variable.  That's what \"scoped by a quantifier\" means. This is important to know when creating well-formed trees but also helps explain some of the uses of other variable types later in this section.\nE (Event) Variables\nEvent variables have a rich history and lot of fascinating conceptual linguistic background to them (Davidson 1967a is a good start), but for our purposes, we can think of them as holding a \"bag of information\" (represented in code as a dictionary, perhaps). Predications introduce them to provide a place for other predications to hang information that will be used by the introducer. \nFor example, event variables are used by adverbs like \"slowly\" as in, \"move slowly\", to provide the move predication with information about how to move. slowly does this by adding data to the event variable that move introduces. You can see in the MRS below for \"move slowly\" that _slow_a_1 is passed the e2 event variable that _move_v_1 introduces:\n[ TOP: h0\nINDEX: e2\nRELS: &lt; \n[ pronoun_q LBL: h4 ARG0: x3 [ x PERS: 2 PT: zero ] RSTR: h5 BODY: h6 ]\n[ pron LBL: h7 ARG0: x3 [ x PERS: 2 PT: zero ] ]\n[ _slow_a_1 LBL: h1 ARG0: e8 [ e SF: prop TENSE: untensed MOOD: indicative PROG: - PERF: - ] ARG1: e2 ]\n[ _move_v_1 LBL: h1 ARG0: e2 [ e SF: comm TENSE: pres MOOD: indicative PROG: - PERF: - ] ARG1: x3 ]\n&gt;\nHCONS: &lt; h0 qeq h1 h5 qeq h7 &gt; ]\n\nThe _slow_a_1 predication is passed the e2 argument so that it can attach data about \"how to do something\" to the event. _move_v_1 needs e2 passed to it so that it can inspect it and determine how to do the \"moving\".  \nEvents can also be used to add information about where to do something. For example, in \"go to the store\", \"to\" is one of many prepositions that can be used with \"go\" to say where to go. So, if a preposition like \"to\" is in the phrase, it modifies the event that \"go\" introduces:\n[ TOP: h0\nINDEX: e2\nRELS: &lt; \n[ _the_q LBL: h10 ARG0: x9 [ x PERS: 3 NUM: sg IND: + ] RSTR: h11 BODY: h12 ]\n[ _store_n_1 LBL: h13 ARG0: x9 [ x PERS: 3 NUM: sg IND: + ] ]\n[ pronoun_q LBL: h4 ARG0: x3 [ x PERS: 2 PT: zero ] RSTR: h5 BODY: h6 ]\n[ pron LBL: h7 ARG0: x3 [ x PERS: 2 PT: zero ] ]\n[ _to_p_dir LBL: h1 ARG0: e8 [ e SF: prop TENSE: untensed MOOD: indicative PROG: - PERF: - ] ARG1: e2 ARG2: x9 ]\n[ _go_v_1 LBL: h1 ARG0: e2 [ e SF: comm TENSE: pres MOOD: indicative PROG: - PERF: - ] ARG1: x3 ]\n&gt;\nHCONS: &lt; h0 qeq h1 h5 qeq h7 h11 qeq h13 &gt; ]\n\nEvent variables conceptually hold a single \"event\" that accumulates information over the course of evaluating the predications. Multiple predications may \"enrich\" it with information before it's actually used by, for example, a verb. Contrast this with an instance (x) variable which only holds a particular individual at a given point it time. Said another way: an instance variable is like a string and can only hold one value, where an event is like a dictionary or a list and can hold many and be added to over time.\nNote that the DELPH-IN grammars are very liberal in putting event variables on predications and, depending on context, sometimes they aren't used. This is just to prevent the consumer of the MRS from having to deal with the same predication both with and without an event variable.\nThe predication that introduces an event variable will often (but not always) be the predication that consumes or \"does something\" with the \"fully enriched\" event. Predications that have it in other arguments will often (but not always) be simply adding information to the event.\nOther Variables Types: I, U, P\nRecall that the variable types in DELPH-IN form a hierarchy. So far we've discussed the bottommost \"leaves\", which are most commonly seen:\n    u\n   / \\\n  i   p\n / \\ / \\\ne   x   h\n\nThe other three types of variables represent a type that is \"in-between\" or \"underspecified\" between the other \"concrete\" types (e, x, h).  In general, these appear when the ERG can't decide the type of something since it falls somewhere between the types (i.e. is \"underspecified\").  From the ERG documentation:\n\"i (for individual) is a generalization over eventualities and instances; p (the half-way mark in the alphabet between h and x) is a generalization over labels and instances; and u (for unspecific or maybe unbound) generalizes over all of the above. Note that Copestake et al. (2001) use individual for what is called instance here.\"\n\nIn practice, they appear in two pretty specific scenarios:\nUnquantified x variables: Some predications in the ERG have an argument that is conceptually an individual (x) type, but does not require quantification. Since the rules require that all x variables are scoped by a quantifier, the most appropriate of the three \"in-between\" types will be used instead as a \"work-around\". This is usually i since these are most often of type x, and i is the most specific of the options that includes x. As with all non-x variables, this will be \"existentially quantified\" (globally defined) -- that is the whole point of using them here.\nDropped arguments: Sometimes the predication that would introduce a variable is missing. For example, take \"I left\" vs. \"I left Oregon\". In the latter, \"Oregon\" becomes a predication that introduces a variable that \"left\" uses, but in the former, this predication doesn't exist, so the variable is not introduced. In this case, the missing (or \"dropped\") variable uses an i, p or u type in place of the original type. Variables typed like this should be treated like the act of passing None in Python or Null in SQL to a function. The easiest way to detect when one of these three variable types means \"dropped or ignored argument\" is by checking if any other predication is also using it (as in the previous case). If not, it is probably dropped/ignored.\n- i means dropped e or x\n- u means dropped e, x, or h\n- p means dropped x or h\nVariable Properties\nVariables in an MRS have properties, which are like single argument predications for the variables. They define many different properties of a variable that aren't included anywhere else. They are defined after the variable in the MRS, surrounded by []. You can see several examples in the MRS for \"he will go\":\n[ pronoun_q LBL: h5 ARG0: x3 [ x PERS: 3 NUM: sg GEND: m IND: + PT: std ] RSTR: h6 BODY: h7 ]\n[ pron LBL: h4 ARG0: x3 [ x PERS: 3 NUM: sg GEND: m IND: + PT: std ] ]\n[ _go_v_1 LBL: h1 ARG0: e2 [ e SF: prop TENSE: fut MOOD: indicative PROG: - PERF: - ] ARG1: x3 ]\n\nThe properties provided depend on the type of variable:\nInstance (x) variables can have these properties:\nNumber (NUM): sg (singular) or pl (plural)\n\nPerson (PERS): 1,2, or 3 for first-person (speaker) I/we, second-person (hearer) you, and third-person otherwise\n\nIndividuated (IND): + or - (meaning true or false). Distinguishes individuated entities introduced by count nouns such as cat or cats from non-individuated referents for mass nouns such as rice\n\nGender (GEN): m for male, f for female, n otherwise\n\nPT: ?\n\nEvent (e) variables can have these properties:\nTense (TENSE): past for past, pres for present, fut for future, or untensed\n\nViewpoint aspect (or 'grammatical aspect') describes the situation from a particular viewpoint, dividing it into endpoints and stages (Smith 1991, 1997)\nPERF (for perfect): + or - (meaning true or false)\n\nPROG (for progressive): + or - (meaning true or false)\n\n\nSentence Force (SF): comm for command, ques for question, prop for proposition. Indicates the type of sentence.\n\nMood (MOOD): Roughly describes the opinions or attitudes of the speaker, with most common values being: subjunctive and indicative\n\nQuantifier Predications\nQuantifiers fill a special role in the MRS (and linguistics in general).  According to Wikipedia:\n\"a quantifier is a type of determiner, such as all, some, many, few, a lot, and no that indicates quantity\". \n\n\"The\" and \"a\" are also really common examples.  That's the kind of description you'd get in a normal \"Learning English\" grammar course, but DELPH-IN uses a much broader definition that includes those examples but adds some more.\nQuantifier predications in DELPH-IN always have a specific argument signature: \nquantifier_q(x,h,h)\n\nIn addition to (often) doing the job of saying \"how much of\" their x variable there should be to make the MRS true (\"lots\", \"some\", \"the\", etc), they provide scope to the x variable. All x variables must be scoped by a quantifier, which means that they can only be used in the branches of the tree that are contained in the quantifier's two h (scopal) arguments. This rule for well-formedness means that there are many quantifiers that don't do \"real\" quantification at all, they are in the MRS solely to scope the x variable. Some also act like \"markers\" of some kind (again without doing any quantification).\nThe MRS for \"go north\" shows an example of this:\n[ TOP: h0\nINDEX: e2\nRELS: &lt; \n[ def_implicit_q LBL: h11 ARG0: x9 [ x PERS: 3 NUM: sg ] RSTR: h12 BODY: h13 ]\n[ place_n LBL: h10 ARG0: x9 [ x PERS: 3 NUM: sg ] ]\n[ _north_a_1 LBL: h10 ARG0: i14 [ i ] ARG1: x9 ARG2: u15 ]\n[ pronoun_q LBL: h4 ARG0: x3 [ x PERS: 2 PT: zero ] RSTR: h5 BODY: h6 ]\n[ pron LBL: h7 ARG0: x3 [ x PERS: 2 PT: zero ] ]\n[ loc_nonsp LBL: h1 ARG0: e8 [ e SF: prop TENSE: untensed MOOD: indicative PROG: - PERF: - ] ARG1: e2 ARG2: x9 ]\n[ _go_v_1 LBL: h1 ARG0: e2 [ e SF: comm TENSE: pres MOOD: indicative PROG: - PERF: - ] ARG1: x3 ]\n&gt;\nHCONS: &lt; h0 qeq h1 h5 qeq h7 h12 qeq h10 &gt; ]\n\n                                \u250c\u2500\u2500 place_n(x9)\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500 and(0,1)\n                    \u2502             \u2514 _north_a_1(i14,x9,u15)\ndef_implicit_q(x9,RSTR,BODY)\n                         \u2502                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500 pron(x3)\n                         \u2514\u2500 pronoun_q(x3,RSTR,BODY)    \u250c\u2500\u2500 loc_nonsp(e8,e2,x9)\n                                                \u2514\u2500 and(0,1)\n                                                         \u2514 _go_v_1(e2,x3)\n\nThe variable x9 represents north but nothing in the phrase is \"quantifying\" direction in any way.  Since the rules for MRS require x variables to be quantified, an abstract quantifier called def_implicit_q is used to do the scoping of the variable.\nNote that, unlike non-quantifier predications, the first (ARG0) argument of a quantifier does not \"introduce\" an \"intrinsic variable\" (as described in the variables section), they just scope and optionally quantify it.\nConstraints\nThe HCONS section of the MRS is used when building a well-formed tree. It puts CONStraints on where the Handles for predications can be validly placed and still be a legal interpretation of the phrase. The only constraints used in \"modern\" MRS are qeq constraints so that's all you'll see in this section.  \nA qeq constraint always relates an h argument of one predication, called a \"hole\", to the handle (LBL:) of another predication. It states that the handle must be a direct or eventual child of the hole in the tree and, if not direct, the only things between the hole and the handle can be quantifiers.  Said a different way: \nA qeq constraint of \"X qeq Y\" says that the direct path from X to Y must only contain quantifiers (except for the final predication Y).\n\nAs we work through fully resolving the MRS into a tree, we'll see more description and examples of how the HCONS section is used.\nIndex\nOne final part of the MRS needs to be described: INDEX:\nTOP: h0\nINDEX: e2\nRELS: &lt; \n[ _the_q__xhh LBL: h10 ARG0: x9 [ x PERS: 3 NUM: sg IND: + ] RSTR: h11 BODY: h12 ]\n[ _cave_n_1 LBL: h13 ARG0: x9 [ x PERS: 3 NUM: sg IND: + ] ]\n[ pronoun_q__xhh LBL: h4 ARG0: x3 [ x PERS: 2 PT: zero ] RSTR: h5 BODY: h6 ]\n[ pron__x LBL: h7 ARG0: x3 [ x PERS: 2 PT: zero ] ]\n[ _to_p_dir__eex LBL: h1 ARG0: e8 [ e SF: prop TENSE: untensed MOOD: indicative PROG: - PERF: - ] ARG1: e2 ARG2: x9 ]\n[ _go_v_1__ex LBL: h1 ARG0: e2 [ e SF: comm TENSE: pres MOOD: indicative PROG: - PERF: - ] ARG1: x3 ]\n&gt;\nHCONS: &lt; h0 qeq h1 h5 qeq h7 h11 qeq h13 &gt; \n\nThe INDEX part of the MRS indicates the variable introduced by the predication (or predications if there is a conjunction) that is the \"main point of the phrase\". It is \"the thing being done\", which is usually the main verb.  In the example above INDEX: e2 is referring to the variable introduced by _go_v_1__ex.  This indicates that the verb go is the \"main point of the phrase\". This is called the \"syntactic head\" in linguistics.\nNote that the INDEX does not always point at a verb. In phrases that just state that something \"is\" something else, such is: \"the flower is blue\", \"is\" is not included. \"blue\" acts like the verb and is the INDEX:\n[ TOP: h0\nINDEX: e2\nRELS: &lt; \n[ _the_q LBL: h4 ARG0: x3 [ x PERS: 3 NUM: sg IND: + ] RSTR: h5 BODY: h6 ]\n[ _flower_n_1 LBL: h7 ARG0: x3 [ x PERS: 3 NUM: sg IND: + ] ]\n[ _blue_a_1 LBL: h1 ARG0: e2 [ e SF: prop TENSE: pres MOOD: indicative PROG: - PERF: - ] ARG1: x3 ]\n&gt;\nHCONS: &lt; h0 qeq h1 h5 qeq h7 &gt; ]\n\n\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500 _flower_n_1(x3)\n_the_q(x3,RSTR,BODY)\n                 \u2514\u2500 _blue_a_1(e2,x3)\n\nMore information on INDEX is described in the section on sentence force.\nThe next topic walks through the rules of creating \"well-formed MRS trees\", and is the last big chunk of conceptual background needed before we start building the system.\nComprehensive source for the completed tutorial is available here.\n\n", "title":"Minimal Recursion Semantics (MRS)", "teaser":"The Minimal Recursion Semantics (MRS) Format\nThis section provides an overview of the Minimal Recursion Semantics format which is the primary artifact ...", "site":"Perplexity", "section":"How-To", "categories":"", "tags":""}
, {"url": "https://blog.inductorsoftware.com/Perplexity/home/devhowto/devhowtoWellFormedTree", "excerpt":"Building Well-Formed MRS Trees\nTo understand this section, first make sure you have a basic understanding of the MRS format.  \n\nLet's use the sentence \"every book is in a cave\" as an example. If the phrase is parsed with the ACE parser, you get an MRS document like this:\n[ TOP: h0\nINDEX: e2\nRELS: &lt; \n[ _a_q LBL: h10 ARG0: x9 [ x PERS: 3 NUM: sg IND: + ] RSTR: h11 BODY: h12 ]\n[ _cave_n_1 LBL: h13 ARG0: x9 [ x PERS: 3 NUM: sg IND: + ] ]\n[ _every_q LBL: h4 ARG0: x3 [ x PERS: 3 NUM: sg IND: + ] RSTR: h5 BODY: h6 ]\n[ _book_n_of LBL: h7 ARG0: x3 [ x PERS: 3 NUM: sg IND: + ] ARG1: i8 ]\n[ _in_p_loc LBL: h1 ARG0: e2 [ e SF: prop TENSE: pres MOOD: indicative PROG: - PERF: - ] ARG1: x3 ARG2: x9 ]\n&gt;\nHCONS: &lt; h0 qeq h1 h5 qeq h7 h11 qeq h13 &gt; ]\n\nOur goal is to eventually \"solve\" the MRS by finding values for its MRS variables such that it is \"true\". When complete, these variables indicate what the speaker meant and allow us to do something about it.  \nTo resolve an MRS against a world state (a particular state of the world at a moment in time) and get solutions to it (meaning the set of MRS variable assignments that make it true) you need to turn it into a well-formed MRS tree. We will examine how shortly, but for now just know that a well-formed MRS tree has (among other things) nodes that are the predications from the MRS like _every_q__xhh and arcs that are links between the scopal arguments of the predications and other nodes, like this:\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500 _book_n_of(x3,i8)\n_every_q(x3,RSTR,BODY)          \u250c\u2500\u2500\u2500\u2500\u2500\u2500 _cave_n_1(x9)\n                   \u2514\u2500 _a_q(x9,RSTR,BODY)\n                                     \u2514\u2500 _in_p_loc(e2,x3,x9)\n\nThis tree represents one interpretation of \"every book is in a cave\", namely, \"every book is in a (possibly different) cave\". \nTo \"solve\" this tree against a particular world state, you walk it in depth-first order: every_q is the starting, leftmost node. It starts by selecting a book on its upper branch, and then solves its lower branch with the selected book. This finds \"a cave that the (selected) book is in\". every_q does this for every book in the world state. If they all succeed (they must all succeed because the speaker said \"every\"), we have a solution to the MRS. Because _every_q chooses a book and then a cave that it is in, it allows a different cave to be selected for each book. This tree will be only true if every book is in a (possibly different) cave.\nBut this is only one interpretation. Another interpretation of the same MRS is: \"all books are in the same exact cave\". The speaker might have meant that interpretation, which is represented by this tree:\n          \u250c\u2500\u2500\u2500\u2500\u2500\u2500 _cave_n_1(x9)\n_a_q(x9,RSTR,BODY)              \u250c\u2500\u2500\u2500\u2500\u2500\u2500 _book_n_of(x3,i8)\n               \u2514\u2500 _every_q(x3,RSTR,BODY)\n                                     \u2514\u2500 _in_p_loc(e2,x3,x9)\n\nWhen a_q is the leftmost node, it starts by selecting a cave on its upper branch, and then resolves its lower branch with that selection, making sure that \"every book is in the (selected) cave\". This will only be true if there is (at least one) cave that every book is in.\nDon't worry if you don't completely understand how the solutions are obtained yet.  The point is that there are different interpretations for the same MRS, represented by different trees. The rest of the tutorial will work through how these get solved.\n\nBoth of these trees are represented by the same MRS document. The MRS structure is said to be underspecified, meaning that a single MRS document allows multiple interpretations. \nHere's the MRS for \"Every book is in a cave\" again, so we can see how:\n[ TOP: h0\nINDEX: e2\nRELS: &lt; \n[ _a_q LBL: h10 ARG0: x9 [ x PERS: 3 NUM: sg IND: + ] RSTR: h11 BODY: h12 ]\n[ _cave_n_1 LBL: h13 ARG0: x9 [ x PERS: 3 NUM: sg IND: + ] ]\n[ _every_q LBL: h4 ARG0: x3 [ x PERS: 3 NUM: sg IND: + ] RSTR: h5 BODY: h6 ]\n[ _book_n_of LBL: h7 ARG0: x3 [ x PERS: 3 NUM: sg IND: + ] ARG1: i8 ]\n[ _in_p_loc LBL: h1 ARG0: e2 [ e SF: prop TENSE: pres MOOD: indicative PROG: - PERF: - ] ARG1: x3 ARG2: x9 ]\n&gt;\nHCONS: &lt; h0 qeq h1 h5 qeq h7 h11 qeq h13 &gt; ]\n\nThe MRS is a flat list of predications so that it avoids building a single tree which would \"lock in\" one interpretation.  How it does this is described in detail next, but in summary: It leaves \"holes\" using scopal (h) arguments for various predications and provides constraints (the HCONS) for plugging the predications together \"legally\".  If you combine the predications by following the constraints (among other things), you'll end up with a \"well-formed MRS tree\" which defines one valid interpretation of the sentence. If you build all the well-formed trees, you have all the possible interpretations.\nThis interpretation is what we need in order to eventually \"solve\" the phrase for the variables it contains. This topic describes how to build that tree.\nHoles and Constraints\n\"Holes\" are h arguments in a predication that refer to a predicate label that is not defined. In the above MRS, h0 (the TOP:), h11, h12, h5, and h6 are all \"holes\" since none of the predicates use those names as their LBL:.\nThe HCONS section of the MRS puts CONStraints on which placement of Handles in holes is valid.\nThe only kind of constraint used in \"modern\" MRS is a qeq constraint.  A qeq constraint always relates a hole to a (non-hole) handle and says that the handle must be a direct or eventual child in the tree. Furthermore, if not directly connected, the only things between the hole and the handle can be quantifiers.  \nSaid a different way: \nA qeq constraint of h0 qeq h1 (as in the above example) says that the direct path in the final tree from h0 to h1 must only contain quantifier predicates, but can contain as many as you want, as long as they don't violate other constraints.\n\nSo, in this MRS:\n[ TOP: h0\nINDEX: e2\nRELS: &lt; \n[ _a_q LBL: h10 ARG0: x9 [ x PERS: 3 NUM: sg IND: + ] RSTR: h11 BODY: h12 ]\n[ _cave_n_1 LBL: h13 ARG0: x9 [ x PERS: 3 NUM: sg IND: + ] ]\n[ _every_q LBL: h4 ARG0: x3 [ x PERS: 3 NUM: sg IND: + ] RSTR: h5 BODY: h6 ]\n[ _book_n_of LBL: h7 ARG0: x3 [ x PERS: 3 NUM: sg IND: + ] ARG1: i8 ]\n[ _in_p_loc LBL: h1 ARG0: e2 [ e SF: prop TENSE: pres MOOD: indicative PROG: - PERF: - ] ARG1: x3 ARG2: x9 ]\n&gt;\nHCONS: &lt; h0 qeq h1 h5 qeq h7 h11 qeq h13 &gt; ]\n\nh1 is the LBL of the _in_p_loc__exx predicate. Given the qeq constraint of h0 qeq h1, it would be perfectly valid to assign h0 = h1 (meaning put the predication labelled by h1 in the h0 hole) since the path from h0 to h1 is direct. \nAgain, given the qeq constraint of h0 qeq h1: You could alternatively assign h0 = h4 (h0 is the \"hole\" at the top of the tree, h4 is the label for _every_q), and h6 = h1 (h6 is a \"hole\" in _every_q, h1 is the label for _in_p_loc). With this configuration, h0 qeq h1 is still valid because the path from h0 to h1 only includes the every_q quantifier and h1 itself.\nOnce you fill all the holes with unique predications, and you follow all of the qeq constraints, you'll end up with a tree that is \"scope-resolved\", but not yet guaranteed to be \"well-formed\". There is one more rule to check.\nX Variable Scoping\nAll of the arguments that aren't handles in the MRS for Every book is in a cave except two (e2 and i8) are x variables:\n[ TOP: h0\nINDEX: e2\nRELS: &lt; \n[ _a_q LBL: h10 ARG0: x9 [ x PERS: 3 NUM: sg IND: + ] RSTR: h11 BODY: h12 ]\n[ _cave_n_1 LBL: h13 ARG0: x9 [ x PERS: 3 NUM: sg IND: + ] ]\n[ _every_q LBL: h4 ARG0: x3 [ x PERS: 3 NUM: sg IND: + ] RSTR: h5 BODY: h6 ]\n[ _book_n_of LBL: h7 ARG0: x3 [ x PERS: 3 NUM: sg IND: + ] ARG1: i8 ]\n[ _in_p_loc LBL: h1 ARG0: e2 [ e SF: prop TENSE: pres MOOD: indicative PROG: - PERF: - ] ARG1: x3 ARG2: x9 ]\n&gt;\nHCONS: &lt; h0 qeq h1 h5 qeq h7 h11 qeq h13 &gt; ]\n\nThe rules for MRS say that any variable in the MRS is \"globally defined\" (or \"existentially qualified\" in logic terms) for the whole structure except for x variables.  So, both e2 and i8 don't need any special handling, they are globally defined.\nx variables, on the other hand, can only be defined by quantifiers, and are only defined for the branches of the tree that are attached to the quantifier's scopal (h) arguments: RSTR and BODY.\nSo, while the predications can be in any order in the tree with respect to their e  (or i or u if it had them) arguments, the tree must be checked to make sure all of the x arguments have an eventual parent which is a quantifier which puts them in scope (i.e. has the x variable as its first argument: ARG0). This is an additional constraint that has to be checked to build a \"well-formed\" tree.\nIf the quantifiers are all in (exactly) one place, the built tree passes all qeq constraints, and the x variables are all properly scoped, then it is a \"well-formed\" tree that we can now attempt to solve.  That's what we're going for here.\nResolving the tree\nFinding ways to efficiently create these trees is an area of active research because natural language can easily create MRS structures that have a ton of holes.  n holes, in the worst case, can require n! checks to resolve, if done exhaustively.  So, an MRS structure with 12 holes (which is easy to generate) could require up to 480,000,000 checks before finding a valid solution if you just try every combination.  \nTo generate the well-formed trees, you could simply try all possible combinations of holes and predications, do the qeq and x scoping checks on each, and only keep the valid ones. This will only be practical for the simplest possible trees.\nAnother algorithm, the one we'll use in the tutorial, is able to prune the search space and works much faster.  The Python implementation can usually generate all trees for an MRS with 12 holes in around 1.5s (with some outliers being slower) on a 2013-era MacBook Pro.  This will be sufficient for the purposes of this tutorial.  Something like \"put the diamond on the table where the safe is and the book is\" generates MRS structures with up to 14 holes and could take up to 30 seconds to generate all the valid interpretations (1500+ valid interpretations in some cases!) for each MRS.  It turns out it is very rarely necessary to generate all the interpretations, but regardless: because it scales factorially, things slow down fast after 12 holes.\nThere are definitely more efficient approaches, but the algorithm below has the advantage of being relatively simple. Here is one alternative.  There are definitely more.\nA Simple, Fast Enough, Algorithm\nIt isn't important to fully understand this algorithm as long as you understand what it has to do: build a well-formed MRS tree, and what the rules are in doing that. We'll use this code as a library routine all throughout the tutorial, but we won't dive into its implementation again. If you've followed along and understood the content so far, you've got enough background to go to the next section where we start to dive into how to implement the predications.\n\nThis description is for those that are interested in how the algorithm works, and isn't necessary for understanding the rest of the tutorial:\nFirst some definitions used in this algorithm:\nHole: A scopal (i.e. h type) argument in an MRS predicate that doesn't refer to an existing predication\n\nFloater: A tree of predications that have had zero or more of their scopal (i.e. h type) arguments filled by unique predications.  [This is not at official MRS term, it is one created for this algorithm]\n\nAs a reminder, a tree is \"well-formed\" if:\nEach floater is assigned to one, and only one, hole. No holes or floaters are left at the end  \n\nNone of the assignments of floaters to holes violates a qeq constraint \n\nAny variable introduced by a quantifier is not used outside of the branches assigned to its RSTR or Body arguments  \n\nHere's the intuition for how the algorithm works: We are going to walk a search tree.  Every node of the search tree represents a partial assignment of floaters to holes that meets the above 3 constraints. Every arc from a parent node in the search tree to a child node in the search tree represents a unique assignment of a (otherwise unassigned) floater to a hole.  If that assignment violates a constraint, the search tree node is not valid (since obviously keeping this assignment and adding floaters to it can't be valid either) and we stop searching that whole branch of the search tree. This pruning is what makes it faster than the really naive \"try every option\" approach. Every node in the search tree that has no holes left to assign is a solution.\nAlgorithm Flow Summary: We start at the TOP: hole and record on it any qeq constraints that apply to it and any X variables that are in scope for it (none at the start). As we traverse an arc in the search tree and assign a new floater to a hole, we propagate any constraints and in-scope variables from the (parent) hole to the holes in the (child) floater.  Then we create the next node in the search tree by choosing the next hole to fill from the existing node.\nStart with:\nEach node in the search tree has the following structures that represent where the search has progressed to:\nallHolesDict:              Dictionary populated with all the holes in the MRS. Each hole has information about:\n- The qeq constraints that currently apply to it\n- The X variables that are currently in scope for it\n- The floater it is from\nnodeAssignmentList:        Assignments of floaters to holes that the search tree node represents. Empty for the initial node.\nnodeRemainingHolesList:    Holes left to fill in this search tree node. Only contains the TOP: hole for the initial node.\nnodeRemainingFloatersList: Floaters still unassigned at this node in the search tree. Contains all floaters for the initial node. Each floater contains information about:\n- A list of holes it contains\n- A list of unresolved x variables it contains\n- A list of any Lo parts of a qeq constraint it contains (if it doesn't also have the Hi part in the floater) \nAlgorithm:\nStarting at the initial node:\nGet currentHole by removing the first hole from nodeRemainingHolesList \n\nGet currentFloater by removing each floater from nodeRemainingFloatersList and: \nIf currentFloater does not violate the constraints in currentHole: \nAdd currentHole = currentFloater to nodeAssignmentList\n\nPropagate the constraints and variables from the new parent to all holes in currentFloater \n\nAdd holes from currentFloater to the end of nodeRemainingHolesList \n\nCheck number of holes left:\nif == 0, return nodeAssignmentList as a solution\n\notherwise, continue the search by \"creating a new search tree node\" via recursing to the top of the algorithm\n\n\n\n\nReturns:\nnodeAssignmentList which is simply a dictionary where the keys are holes and the value is the floater that was assigned to it.\nOnce this has run its course you will have all the valid well-formed trees for the MRS. \nHere is the Python code for the main routine:\ndef TryAlternativeHoleAssignments(allHolesDict, nodeRemainingHolesListOrig, nodeRemainingFloatersList, nodeAssignmentList):\n    # Grab the first hole to fill and remove it from the list\n    currentHole = allHolesDict[nodeRemainingHolesListOrig[0]]\n    nodeRemainingHolesList = nodeRemainingHolesListOrig[1:]\n\n    index = 0\n    # Try each remaining floater in this hole\n    for index in range(0, len(nodeRemainingFloatersList)):\n        # Grab the current floater and pull from the list for when we recurse\n        currentFloater = nodeRemainingFloatersList[index]\n        newNodeRemainingFloatersList = [x for i, x in enumerate(nodeRemainingFloatersList) if i != index]\n\n        # Check if constraints are met. If not, prune entire search space by\n        # skipping since none of its children can work either\n        errorOut = []\n        if not CheckConstraints(currentHole[&quot;Constraints&quot;], currentFloater, errorOut):\n            # Constraint Failed: try the next one\n            continue\n\n        # Hole successfully filled\n        # Assign the floater to the hole in a copy of assignments since we will be\n        # changing on each loop\n        currentAssignments = copy.deepcopy(nodeAssignmentList)\n        currentAssignments[currentHole[&quot;Label&quot;]] = currentFloater[&quot;Label&quot;]\n\n        if len(newNodeRemainingFloatersList) == 0:\n            # We filled the last hole, return the solution\n            yield currentAssignments\n            return\n\n        # If this floater has more holes, add them to a copy of the nodeRemainingHolesListOrig\n        # Fixup any of the holes from this floater in a *copy* of holeDict since it also holds the holes\n        # and the pointer to the hole is being changed so we don&#x27;t want other nodes to get changed too\n        newNodeRemainingHolesList = copy.deepcopy(nodeRemainingHolesList)\n        newHoleDict = copy.deepcopy(allHolesDict)\n        FixupConstraintsForFloaterInHole(currentHole[&quot;Constraints&quot;], currentFloater, newHoleDict)\n        for nextHoleName in currentFloater[&quot;FloaterTreeHoles&quot;]:\n            newNodeRemainingHolesList.append(nextHoleName)\n\n        # This hole was filled, see if any remain\n        if len(newNodeRemainingHolesList) &gt; 0:\n            # recurse\n            yield from TryAlternativeHoleAssignments(newHoleDict, newNodeRemainingHolesList, newNodeRemainingFloatersList, currentAssignments)\n\n    # At this point we tried all the floaters in this hole\n    return\n\nComprehensive source for the completed tutorial is available here.\n\n", "title":"Well-Formed Trees", "teaser":"Building Well-Formed MRS Trees\nTo understand this section, first make sure you have a basic understanding of the MRS format.  \n\nLet's use the sentence ...", "site":"Perplexity", "section":"How-To", "categories":"", "tags":""}
, {"url": "https://blog.inductorsoftware.com/Perplexity/home/devhowto/devhowtoMRSSolver", "excerpt":"It is important to understand what MRS is and what a well-formed MRS tree is before reading this section. Visit those links first to understand the basic concepts.\n\nA well-formed MRS tree can be thought of as an equation that is true or false depending on the values of its variables. Recall that predications are of the form: _table_n_1(x) or compound(e,x,x). Just like functions in mathematics or programming languages, they have a name and a set of arguments. They are true when their arguments are set to values that are or mean what the predication means. \nSo:\nIf we set x = 'a cat' then _table_n_1(x) will be false\n\nIf we set x = 'the brown table' then _table_n_1(x) will be true\n\n_table_n_1(x) means: \"the object in x is a table\". It might also be many other things, like brown or large or missing, etc. But as long as it is at least a table, _table_n_1(x) is true.\nHow a cat or the brown table are actually represented doesn't matter as long as the predication can interpret it. It could be a string or an object of some kind.\nA group of multiple predications separated by commas means they are \"in conjunction\", which means the commas should be treated as and. So,large(x), file(x) will only be true if x is set to values that make all the predications true. In this example, x must be set to something that is a large file. Again, x could be a large yellow file or a large file of paperwork, but each predication just tests for some property of x and is true as long as that property is true, regardless of its other properties.\nSolving a well-formed tree means finding values for the variables that make all the predications true.\nHere's a simple example. Let's solve large(x), file(x) in a world with 3 objects in it:\na folder\na small file\na large file\n\nx variables in an MRS represent \"individuals\" or \"things in the world\". So, we need to find the values of x that make both the predications in large(x), file(x) be true. \nWhile this is trivial to solve by looking at it, once the world contains a million objects, we need a more systematic approach.\nA Backtracking MRS Solver\nWe can look at solving an MRS as a constraint satisfaction problem which is a well-studied class of problems that have a finite set of constraints over variables. In the MRS case, the constraints are the predications:\nlarge(x) constrains x to only be those objects that are large\n\nfile(x) constrains x to only be those objects that are a file\n\nlarge(x), file(x) constrains x to be a large file\n\nOne simple approach to solving constraint satisfaction problems (like the solutions to an MRS) is to use \"backtracking\". The simplest backtracking algorithm is to:\nTraverse the predications from the well-formed MRS tree, depth first\n\nWhen we encounter a variable in a predication that is unassigned we: \nAssign it the first item in the world\n\nMark it as a backtrack point\n\n\nIf a predication is false, we:\n\"backtrack\" to the nearest backtrack point and retry with the next item in the world\n\nIf we run out of items, we backtrack further to the next backtrack point and try again. \n\n\nThis will try all items in the world, in all variables, until it finds all solutions. Backtracking allows the search space to be pruned to avoid whole sets of assignments that can't possibly work, thus improving the performance.\nLet's use the backtracking algorithm to solve a slightly more interesting example, \"large file in folder\":\n[It is important to note that these are not real MRS or well-formed tree examples yet!  We are building up to that.]\nformula: large(x), file(x), folder(y), in(x, y)\n\nworld individuals:\na folder\na small file\na large file\n\nworld facts:\n[a large file] is in [a folder]\n\nThe \"world individuals\" above are the only objects that exist in the world. x values in MRS will hold these as values.\nThe \"world facts\" above are facts about the relationships between things in the world that predications (such as in(x, y)) can refer to to see if they are true.\nAs above, it doesn't matter how either of these is actually represented in a program, as long as the predications know how to find and interpret them. We'll be building an example of such a system in this tutorial.\nTo make the backgracking algorithm more explicit, and to make the formula more like real MRS predications, we need to introduce a notion of \"variable scope\" to the formula. Variable scope shows where a variable is introduced and which predications can use it. \nWe'll represent scope by a function for now: scope(variable, [predication_list]). The function states that variable can be used by all the predications in [predication_list]. And, since scope() itself is a predication, more variables can be defined in predication_list using another scope(). This allows us to represent our formula using scoping, like this:\nformula: scope(x, [large(x), file(x), \n                                      scope(y, [folder(y), in(x, y)])\n                  ])\n\nThe formula is formatted to make it easier to see the nesting. You can see that this is just a flat way of representing a tree shaped like this:\n                            \u250c\u2500\u2500 large(x)\n                            \u2502 \u250c\u2500\u2500 file(x) \n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500 and(0,1,2)\n                \u2502               \u2514\u2500\u2500 scope(y, predication_list)\nscope(x, predication_list)                          \u2502\n                                                    \u2514\u2500 and(0,1)\n                                                           \u2502 \u2514 in(x, y)\n                                                           \u2514 folder(y)\n\n... where and(...) has been used to explicitly show the conjunctions (i.e. ands).\nThe backtracking algorithm does its job by recursively \"evaluating\" the scope() predications:\nEvaluating a scope() means:\nAssign a world value to the scope's variable\n\nEvaluate its predication_list using that value (see below for how)\n\nIf the list is false, restart the list using the next world value\n\nIf the list is true, the scope() predication is true\n\nEvaluating a predication_list means:\nEvaluate the first predication in the list using the current values of all variables in scope\n\nIf true, try the next predication in the list\n\nIf false, the list is false\n\nIf all the predications are true, the list is true\n\n\nSo, working through the example:\n|action|formula|\n|---|---|\n|Initial formula |    scope(x, [large(x), file(x), scope(y, [folder(y), in(x, y)])]) |\n|set x='a folder' (the first item in the world) |    scope('a folder', [large('a folder'), file('a folder'), scope(y, [folder(y), in('a folder', y)])]) |\n|first item in list is false|    ... large('a folder')...|\n|backtrack: set x='a small file' (the next item in the world) |    scope('a small file', [large('a small file'), file('a small file'), scope(y, [folder(y), in('a small file', y)])]) |\n|first item in list is false|    ... large('a small file')...|\n|backtrack: set x='a large file' |    scope('a large file', [large('a large file'), file('a large file'), scope(y, [folder(y), in('a large file', y)])]) |\n|first item in list is true|    ... large('a large file')...|\n|second item in list is true|    ... file('a large file')...|\n|third item in list is scope(): set y='a folder' (the first item in the world)|    ...  scope('a folder', [folder('a folder'), in('a large file', 'a folder')])|\n|first item in scope(y, ...) is true|    ... folder('a folder')...| |second item in scope(y, ...) is true|    ... in('a large file', 'a folder')...|\n|thus: scope(y, ...) is true for y='a folder'|    ...  scope('a folder', [folder('a folder'), in('a large file', 'a folder')])|\n|thus: scope(x, ...) is true for x='a large file' and y='a folder'|scope('a large file', [large('a large file'), file('a large file'), scope('a folder', [folder('a folder'), in('a large file', 'a folder')])])|\nThis example shows how:\nIteratively assigning values to each variable in a scope\n\nEvaluating the predication list within a scope\n\nBacktracking when there is a failure\n\n... will eventually find all the solutions to the formula (or prove that there are none). \nIt works because we are effectively trying all values in all variables. But, it is better than literally just assigning all values to all variables, one by one, until we find the answer, because backtracking eliminates whole branches in the search space. There are other optimizations that can be done, and we will do more as we go, but the basic approach is straightforward.\nAt this point it should be noted that there are other algorithms for solving constraint satisfaction problems. Furthermore, the MRS tree can sometimes be transformed into other forms, such as a predicate logic formula, and solved using other algorithms. This tutorial will be using the backtracking algorithm because it is simple, efficient enough for many problems and has the nice property that it can handle all MRS formulas. It has the downside that it can be very inefficient in some cases. We'll work through some of those and find optimizations for some of the most aggregious problems.\nBefore we can solve a real well-formed MRS tree, we need to account for more of the features that it has.\n", "title":"Solving an MRS", "teaser":"It is important to understand what MRS is and what a well-formed MRS tree is before reading this section. Visit those links first to understand the ba ...", "site":"Perplexity", "section":"How-To", "categories":"", "tags":""}
, {"url": "https://blog.inductorsoftware.com/Perplexity/home/devhowto/devhowtoPlurals", "excerpt":"Algorithm for Dealing with Plurals in MRS\nOverview\nA really brief summary of one possible approach for properly resolving plurals in an MRS against a world state is:\nThe fully resolved tree for an MRS can be evaluated against a world state in two stages:\nStage 1: Remove all the numeric determiner semantics (\"many\", \"2 or more\", \"all\", \"some\", \"the\", etc.) from the fully-resolved tree and solve it. This involves literally removing the numeric adjective determiners and their modifiers (e.g. card(2,e,x) or much-many_a(e8,x3)) and converting the numeric quantifier determiners (e.g. _all_q(x3,RSTR,BODY) or _the_q(x3,RSTR,BODY)) to udef_q. This creates a set of \"undetermined solutions\".\nStage 2: Create groups out of the undetermined solutions that satisfy the first determiner and run each group recursively (left to right) through the rest of the numeric determiners in order. The groups that succeed are solutions.  Forward and reverse readings happen via different fully-resolved trees.\n\nBelow I walk through this in much more detail, with examples.\nCollective/Distributive/Cumulative Definitions\nFor \u201ctwo firefighters carried two hoses\u201d:\nWhen referring to a fully-resolved tree (which means that forward and reverse readings with respect to word order are just different trees and aren\u2019t included in the definition):\n\u201cThe distributive reading\u201d\nFirefighters: 2 or more subgroups, each of size > 0, where every individual in question is in exactly one group (the definition above).\n\nHoses: All subgroups must have two hoses each. Individual hoses may be repeated in subgroups.\n\n\u201cThe collective reading\u201d\nFirefighters: Exactly 1 subgroup that contains the entire set of individuals in question\n\nHoses: Identical to distributive\n\n\u201cThe cumulative reading\u201d\nFirefighters: Identical to distributive\n\nHoses: The total of unique individual hoses across all subgroups adds up to two.\n\nSo:\nDistributive and collective group the first variable differently, but do the same math problem across the group(s) for the second variable.\nCumulative does the same first variable grouping as distributive, but a different math problem across the groups for the second variable.\nDeterminer Definitions\nA numeric determiner creates a numeric constraint on a particular x variable\n\nA numeric adjective determiner is an adjective that creates a numeric constraint on a particular x variable, such as card(2,e,x) or much-many_a(e8,x3).\n\nA numeric quantifier determiner is a quantifier that creates a numeric constraint on a particular x variable, such as _all_q(x3,RSTR,BODY) or _the_q(x3,RSTR,BODY)\n\nStage 1\nStage 1 is done by removing all the numeric determiner semantics from a fully-resolved MRS tree and then evaluating it. For example, here's a forward reading (with respect to word order) of: \"some men are eating two pizzas\":\n_some_q(x3,_man_n_1(x3),udef_q(x10,[_pizza_n_1(x10), card(2,e14,x10)],_eat_v_1(e2,x3,x10)))\n\nIn stage 1, _some_q is converted to udef_q (which seems like the most \"NOOP\" quantifier) and card is removed, like this:\nudef_q(x3,_man_n_1(x3),udef_q(x10,_pizza_n_1(x10),_eat_v_1(e2,x3,x10)))\n\n... which gets you something like \"men are eating pizzas\", but, I evaluate it ignoring the plural as well, so, it is really more like: \"man/men is/are eating pizza(s)\".  Solving that gives you a flat set of solutions that I'm calling \"undetermined solutions\". \nFor example, imagine a world where a bunch of men are eating pizzas and we solve the \"undetermined MRS\" above for all values of x3 and x10. Below, solutions are represented as assignments of values (which are always a set but can be a set of 1) to variables in the MRS that make it true. The set of variable assignments in 1 solution makes the MRS true, and the MRS is true for each of the solutions. Within a solution: If a variable is assigned a set of > 1, it means those individuals are doing something \"together\". \nUsing that approach, here is a list of all the true states of the \"undetermined MRS\" in this world, found by resolving the tree (how that is done is described elsewhere, to be written):\nSolution 1: x3=[man1], x10=[pizza1]                 &quot;man1 is eating pizza1&quot;\nSolution 2: x3=[man2], x10=[pizza2]\nSolution 3: x3=[man3], x10=[pizza3]\nSolution 4: x3=[man3], x10=[pizza4]\nSolution 5: x3=[man4], x10=[pizza5]\nSolution 6: x3=[man4], x10=[pizza6]\nSolution 7: x3=[man5,man6], x10=[pizza7]            &quot;man5 and man6, together, are eating pizza7&quot;\nSolution 8: x3=[man5,man6], x10=[pizza8]\nSolution 9: x3=[man7,man8], x10=[pizza9, pizza10]   &quot;man7 and man8, together are eating pizza9 and pizza10 at the same time&quot;\nSolution 10: x3=[man9], x10=[pizza11, pizza12]      &quot;man9 is eating pizza11 and pizza12 at the same time&quot;\nSolution 11: x3=[man10,man11], x10=[pizza13]\nSolution 12: x3=[man12], x10=[pizza14]\n\nAt the end of Stage 1, we have just the flat list of undetermined solutions.\nStage 2\nSummary: Stage 2 recursively (left to right in execution order) runs each numeric determiner that has been removed over a group of solutions generated by the previous determiner. \nThe first determiner creates a \"determiner group\" for every combination of solutions that satisfy it and sends each determiner group forward to the next.\n\nThe next determiner checks to see if the entire provided determiner group satisfies it. If so, it passes it forward to the next.\n\netc.\n\nAny groups that make it all the way to the end are a valid interpretation\n\nSo, the first determiner, some_q(x3, ...), has the special job of creating all the determiner groups that will be tested by the rest of the determiners. It groups the incoming initial solutions (the flat list of undetermined solutions, above) into all possible groups where there are, lets say, at least two x3 individuals (meaning \"some >= 2\"), like this:\nsome_q(x3, ...) determiner groups\n-------------------------------\n\nGroup 1: (created since 2 men)\n    x3=[man1]:\n        Solution 1: x3=[man1], x10=[pizza1]\n    x3=[man2]:\n        Solution 2: x3=[man2], x10=[pizza2]\n\nGroup 2: (created since 2 men - only unique values matter)\n    x3=[man3]:\n        Solution 3: x3=[man3], x10=[pizza3]\n        Solution 4: x3=[man3], x10=[pizza4]\n    x3=[man4]:\n        Solution 5: x3=[man4], x10=[pizza5]\n        Solution 6: x3=[man4], x10=[pizza6]\n\nGroup 3: (created since 2 men - unique *individuals* are counted)\n    x3=[man5,man6]:\n        Solution 7: x3=[man5,man6], x10=[pizza7]\n        Solution 8: x3=[man5,man6], x10=[pizza8]\n    \nGroup 4: (created since 3 men is still &quot;some&quot;)\n    x3=[man7,man8]:\n        Solution 9: x3=[man7,man8], x10=[pizza9, pizza10]\n    x3=[man9]:\n        Solution 10: x3=[man9], x10=[pizza11, pizza12]\n\nGroup 5: (created since 3 men is still &quot;some&quot;)\n    x3=[man10,man11]:\n        Solution 11: x3=[man10,man11], x10=[pizza13]\n    x3=[man12]:\n        Solution 12: x3=[man12], x10=[pizza14]\n\n... etc. (there are *many* more solutions not listed)\n\nThen, stage 2 tests the next determiner against each generated determiner group. Now that we're going beyond the initial determiner, stage 2 evaluation no longer generates new groups, it just tests them. It also has to use one additional test to find all the collective, distributive and cumulative answers. So, there are two tests used by every determiner beyond the first.\nTest 1\nTest 1 is the new one (not used by the initial determiner) and tests if the total for each previous determiner variable value satisfies its determiner. It will find distributive answers (among others).\nSo, If you take each incoming some_q(x3, ...) determiner group like Group 1, and evaluate card(2,e14,x10) against the solutions that go with unique x3 values, you get these results:\nGroup 1: (FAIL: each unique x3 doesn&#x27;t have 2 x10 pizzas)\n    x3=[man1]:\n        Solution 1: x3=[man1], x10=[pizza1]\n    x3=[man2]:\n        Solution 2: x3=[man2], x10=[pizza2]\n\nGroup 2: (distributive over x3: each unique x3 has 2 x10 pizzas)\n    x3=[man3]:\n        Solution 3: x3=[man3], x10=[pizza3]\n        Solution 4: x3=[man3], x10=[pizza4]\n    x3=[man4]:\n        Solution 5: x3=[man4], x10=[pizza5]\n        Solution 6: x3=[man4], x10=[pizza6]\n\nGroup 3: (collective over x3: each unique x3 has 2 x10 pizzas)\n    x3=[man5,man6]:\n        Solution 7: x3=[man5,man6], x10=[pizza7]\n        Solution 8: x3=[man5,man6], x10=[pizza8]\n    \nGroup 4: (distributive over x3: each unique x3 has 2 x10 pizzas)\n    x3=[man7,man8]:\n        Solution 9: x3=[man7,man8], x10=[pizza9, pizza10]\n    x3=[man9]:\n        Solution 10: x3=[man9], x10=[pizza11, pizza12]\n\nGroup 5: (FAIL: each unique x3 doesn&#x27;t have 2 x10 pizzas)\n    x3=[man10,man11]:\n        Solution 11: x3=[man10,man11], x10=[pizza13]\n    x3=[man12]:\n        Solution 12: x3=[man12], x10=[pizza14]\n\nTest 1 succeeds when the count of individuals in its characteristic variable per previous determiner group x3 subset satisfy its determiner. Thus, in general, it finds the following readings over the previous quantifier:\nAll distributive readings (e.g. Group 2) since it checks the \"per x3 total\", and when all the x3 values are a single individual, that's the definition of distributive.\n\nAll collective readings (e.g. Group 3) since it will test a \"per x3 total\" for a single x3 set > 1 \n\nSome tiny number of technically cumulative readings where the x10 individuals are the same for each x3\n\nTest 2\nTest 2 is the test used by the initial determiner and is the \"cumulative\" test: it tests if the total across the whole determiner group satisfies its determiner. \nFor Test 2: If you take Group 1 from some_q(x3, ...) and test the card(2,e14,x10) determiner across the whole group (i.e. ignoring its x3 subsets), you've done one group. If you do them all, you'll get:\nGroup 1: (cumulative over x3: count of all unique x10 is 2)\n    Solution 1: x3=[man1], x10=[pizza1]\n    Solution 2: x3=[man2], x10=[pizza2]\n\nGroup 2: (FAIL: count of all unique x10 is &gt; 2)\n    Solution 3: x3=[man3], x10=[pizza3]\n    Solution 4: x3=[man3], x10=[pizza4]\n    Solution 5: x3=[man4], x10=[pizza5]\n    Solution 6: x3=[man4], x10=[pizza6]\n\nGroup 3: (collective over x3: count of all unique x10 is 2)\n    Solution 7: x3=[man5,man6], x10=[pizza7]\n    Solution 8: x3=[man5,man6], x10=[pizza8]\n    \nGroup 4: (FAIL: count of all unique x10 is &gt; 2)\n    Solution 9: x3=[man7,man8], x10=[pizza9, pizza10]\n    Solution 10: x3=[man9], x10=[pizza11, pizza12]\n\nGroup 5: (cumulative over x3: count of all unique x10 is 2)\n    Solution 11: x3=[man10,man11], x10=[pizza13]\n    Solution 12: x3=[man12], x10=[pizza14]\n\nTest 2 finds the following readings over the previous quantifier:\nAll collective readings (e.g. Group 3). In a collective reading like Group 3, there is only one value of x3 (that happens to be a set > 1) and thus there is no distinction between \"per x3 value\" and \"across all x3 values\", so this test duplicates all collective groups from Test 1.\n\nAll cumulative readings (e.g. Group1 and Group 5).\n\nSome (tiny number of technically) distributive readings. Namely, ones where each x3 meets the determiner test with the exact same values. These will be duplicates of Test 1.\n\nNote that, if you limit test 2 to only groups that have exactly 1 unique x3 assignment, then the only duplication that happens is the distributive one listed, with no loss of generality.\nAfter running through both Phase 1 and Phase 2 for each determiner, in order, the groups that remain represent all the collective, distributive and cumulative solutions, with some duplication.\n", "title":"Plural Algorithm", "teaser":"Algorithm for Dealing with Plurals in MRS\nOverview\nA really brief summary of one possible approach for properly resolving plurals in an MRS against a  ...", "site":"Perplexity", "section":"How-To", "categories":"", "tags":""}
]
